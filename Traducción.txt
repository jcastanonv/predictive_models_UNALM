#El conjunto de datos en cuestión trata sobre la lesión hepática (daño hepático)
#Incluye un marco de datos de predictores biológicos relacionados con el daño
#hepático bio, un marco de datos de predictores químicos relacionados con la 
#chem y la variable de respuesta que nos interesa, la lesión. Si un modelo 
#se puede ajustar adecuadamente, ese modelo podría usarse para detectar compuestos
#dañinos en el futuro.

library(caret)
library(pROC)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(hepatic)

# Antes de ajustar un modelo, siempre es necesario preprocesar los datos.
# Para los algoritmos de clasificación lineal, esto significa

#1.Remove near zero variance predictor variables/ near-zero variance) predictors.
#2.Remove collinear variables

## Eliminar las variable problemática para el 'bio' dataframe
bio_zerovar <- nearZeroVar(bio)
bio_collinear <- findLinearCombos(cov(bio))$remove
bio <- bio[, -c(bio_zerovar, bio_collinear)]

##Eliminar las variable problemáticas para 'chem' dataframe
chem_zerovar <- nearZeroVar(chem)
chem_collinear <- findLinearCombos(chem)$remove
chem <- chem[, -c(chem_zerovar, chem_collinear)]

#La variable de respuesta a la lesión a la que me adapto tiene tres clases
#: "Ninguna", "Leve" y "Severa". Si la variable de respuesta tiene demasiadas 
#clases, se puede recortar (algo subjetivamente). Por conveniencia, decido 
#dividir las lesiones en 2 clases: "Sí" o "No", donde cuento las lesiones leves 
#como "No". (Advertencia: esto puede influir negativamente en la predicción, 
#por lo que en el futuro me aseguraré de probar las predicciones de probabilidad
#de varias clases).

##Contraer respuesta en valores "Yes" o "No"
lut <- c("None" = "No", "Mild" = "No", "Severe" = "Yes")
injury2 <- factor(unname(lut[injury]))

# Ahora deberia considerar otras preguntas

#1. Como particionar la base con clases no balanceadas (“No” supera con creces 
#los valores de “Sí” en lesiones.)

#2. Como validar los resultados del modelo

#3. qué métrica maximiza el mejor modelo

# Por suerte con la función "createDataPartition" se puede realizar
# la partición de la base automaticamente usando muestras estratificadas para 
# Clases no balanceadas.

# Para validación se decide que el modelo tenga una valdiación cruzada con 
# 10 folds repetitivos la cual será especificado en el "traincontrol".
# Aunque el primer modelo no necesitará validación cruzada, los demás sí por ello
# se usará el mismo control para cada modelo.

set.seed(1234)

## División de la base con muestra estratificada 

training <- createDataPartition(injury2, p = .8, list = FALSE)

## División de la base de entrenamiento y prueba para BIO

bio_train <- bio[training, ]

bio_test <- bio[-training, ]


## División de la base de entrenamiento y prueba para Chem

chem_train <- chem[training, ]

chem_test <- chem[-training, ]

##División de la base de entrenamiento y prueba para la variable respuesta 

injury_train <- injury2[training]

injury_test <- injury2[-training]

## Establecer control de entrenamiento para la construcción del modelo

ctrl <- trainControl(method = "repeatedcv", 10, repeats = 10, 
                     summaryFunction = twoClassSummary, classProbs = TRUE, 
                     savePredictions = TRUE)


# Es importante decidir el que objetivo tendrá las métricas de Accuracy, 
# Sensitivity, or Specificity tendrá el modelo. Para ello es necesario saber cual 
# es la variable "positiva" desde la perspectiva de caret.

#Caret escoje el primer clase de factor como positivo la cual corresponde a "no"
#en el vector "injury". Por lo tanto, Sensitivity corresponde a la cantidad de 
#valores "no" correctamente predichos, mientras que la  Specificity corresponde
# a los valores "si" correctamente predichos.


# decidir entre esas opciones, parece más importante construir un modelo que 
# pueda predecir los valores "yes" en el caso resulte un daño hepático.
# Esto tiene sentido, un error en un modelo que predice que no hay daño hepático 
#sería trágico, por lo que deberíamos hacer todo lo posible para capturar
#los valores de "Sí" tanto como sea posible, incluso si eso significa sacrificar 
#la precisión. Una mirada a los datos:

#El primer modelo es un clasificador que utiliza análisis discriminante lineal. 
#Aplico un modelo tanto para los indicadores biológicos como para los indicadores 
#químicos, para ver cuáles tienen mejor poder predicativo.

bio_lda <- train(bio_train, injury_train, method = "lda", trControl = ctrl, metric = "Spec")

chem_lda <- train(chem_train, injury_train, method = "lda", trControl = ctrl, metric = "Spec")

##Genere predicciones del modelo LDA para el conjunto de pruebas de indicadores biológicos

bio_lda_pred <- predict(bio_lda, bio_test)

## Generar la matriz de confunsión para mostrar los resultados

confusionMatrix(bio_lda_pred, injury_test, positive = "No")

#Tenga en cuenta que la "tasa de no información" es .89, lo que significa que
#si adivinamos "No" al azar cada vez, el modelo automáticamente acertaría el 89%
#de las veces. La precisión aquí es .83, que parece tener un rendimiento inferior.
#Pero recuerde, la precisión no es importante; predecir correctamente los valores 
#verdaderos de "Sí" sí lo es.


#Para los predictores biológicos, obtenemos .5 para Especificidad, 
#identificando correctamente 3 valores de Sí pero también generando 3
#predicciones falsas negativas para otros 3 valores de Sí; espero que otros 
#modelos puedan hacerlo mejor.

##Chem predictor LDA model
chem_lda_pred <- predict(chem_lda, chem_test)
confusionMatrix(chem_lda_pred, injury_test, positive = "No")

#La LDA para predictores químicos tiene peores resultados en la predicción de
#los verdaderos valores de Sí. Aquí la especificidad solo alcanza .16.

#Ahora probemos algunos otros modelos para comparar. El primero será un modelo
#de regresión logística penalizado. Para valores alfa de 1 y lambda 0, se
#comportará como un modelo de lazo, mientras que con alfa 0 y una lambda distinta
#de cero, un modelo de regresión de cresta. En cualquier lugar intermedio hay 
#una red elástica. Aquí, no especifico una cuadrícula de ajuste, simplemente 
#dejo que Caret me presente una lista de parámetros, con 'tuneLength = 20'.

bio_plr <- train(bio_train, injury_train, method = "glmnet", family = "binomial",
                 metric = "Spec", tuneLength = 20,trControl = ctrl)

chem_plr <- train(chem_train, injury_train, method = "glmnet", family = "binomial",
                  metric = "Spec", tuneLength = 20, trControl = ctrl)

bio_plr_pred <- predict(bio_plr, bio_test)

confusionMatrix(bio_plr_pred, injury_test, positive = "No")


#Esta regresión logística penalizada no funciona tan bien como el análisis 
#discriminante lineal simple para los predictores biológicos.

#Y ahora los predictores químicos.

chem_plr_pred <- predict(chem_plr, chem_test)
confusionMatrix(chem_plr_pred, injury_test, positive = "No")

#La regresión logística penalizada es aún peor para los predictores químicos. 
#Claramente, este no es un modelo sólido para capturar la estructura de los 
#datos para el patrón que estamos buscando.

#Ahora, para ajustar un modelo de regresión de mínimos cuadrados parciales.

bio_pls <- train(bio_train, injury_train, method = "pls", trControl = ctrl, metric = "Spec", tuneLength = 20)
chem_pls <- train(chem_train, injury_train, method = "pls", trControl = ctrl, metric = "Spec", tuneLength = 20)
bio_pls_pred <- predict(bio_pls, bio_test)
confusionMatrix(bio_pls_pred, injury_test, positive = "No")

#Solo .16 para la especificidad lograda aquí.

#Y para los predictores químicos, obtenemos 0 como se ve a continuación.
#A los indicadores químicos continuamente les está yendo peor que a los
#predictores biológicos para predecir la lesión hepática, y todavía no existe 
#un gran modelo, hasta ahora.

chem_pls_pred <- predict(chem_pls, chem_test, probability = TRUE)
confusionMatrix(chem_pls_pred, injury_test, positive = "No")


bio_centroid <- train(bio_train, injury_train, method = "pam",
                      trControl = ctrl, preProcess = c("center", "scale"), 
                      metric = "Spec", tuneLength = 20)

chem_centroid <- train(chem_train, injury_train, method = "pam",
                       trControl = ctrl, preProcess = c("center", "scale"), 
                       metric = "Spec", tuneLength = 20)

bio_centroid_pred <- predict(bio_centroid, bio_test)
chem_centroid_pred <- predict(chem_centroid, chem_test)
confusionMatrix(bio_centroid_pred, injury_test, positive = "No")

#0 para el indice de especificidad.

#And .16. for Specificity here, yawn.

#Entonces, el mejor modelo para predecir la lesión hepática resulta ser el primer ajuste,
# el modelo LDA sobre los indicadores biológicos.

predictions_bio_lda <- predict(bio_lda, bio_test, type = "prob")
pROC::plot.roc(injury_test, predictions_bio_lda$Yes)

#Sin embargo, el área debajo de la curva no es tan alta como quisieramos. 
#Quizás en el futuro sea revisado estos datos y con la finalidad de que se opte por una  
#diferente manera de predecir lesiones.
